{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing using pandas and scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "Data preprocessing is most always the step before training a machine learning model. There are features that are not very useful for predicting a given outcome. For example, including an `id` field which uniquely identifies each sample does not make much sense. \n",
    "Thus, such variables can be safely deleted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X', 'Y', 'Unique Squirrel ID', 'Hectare', 'Shift', 'Date',\n",
       "       'Hectare Squirrel Number', 'Age', 'Primary Fur Color',\n",
       "       'Highlight Fur Color', 'Combination of Primary and Highlight Color',\n",
       "       'Color notes', 'Location', 'Above Ground Sighter Measurement',\n",
       "       'Specific Location', 'Running', 'Chasing', 'Climbing', 'Eating',\n",
       "       'Foraging', 'Other Activities', 'Kuks', 'Quaas', 'Moans', 'Tail flags',\n",
       "       'Tail twitches', 'Approaches', 'Indifferent', 'Runs from',\n",
       "       'Other Interactions', 'Lat/Long'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "filename=\"../../additional_resources/datasets/NYC Squirrels/data.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll ask you do to a bit of work yourself. This time, we ask you to drop unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X', 'Y', 'Hectare', 'Shift', 'Date', 'Hectare Squirrel Number', 'Age',\n",
       "       'Primary Fur Color', 'Highlight Fur Color',\n",
       "       'Combination of Primary and Highlight Color', 'Color notes', 'Location',\n",
       "       'Above Ground Sighter Measurement', 'Specific Location', 'Running',\n",
       "       'Chasing', 'Climbing', 'Eating', 'Foraging', 'Other Activities', 'Kuks',\n",
       "       'Quaas', 'Moans', 'Tail flags', 'Tail twitches', 'Approaches',\n",
       "       'Indifferent', 'Runs from', 'Other Interactions', 'Lat/Long'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the `Unique Squirrel ID'` column\n",
    "df = df.drop(\"Unique Squirrel ID\",axis =1)\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature slicing\n",
    "Feature slicing is the act of *slicing* a feature into multiple different features.\n",
    "For example, we can slice the `Date` into day, month, and year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10142018\n",
       "1    10062018\n",
       "2    10102018\n",
       "3    10182018\n",
       "4    10182018\n",
       "Name: Date, dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Date'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: use the `Series.apply()` method with a lambda function. [[Help]](https://www.analyticsvidhya.com/blog/2020/03/what-are-lambda-functions-in-python/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       10142018\n",
       "1       10062018\n",
       "2       10102018\n",
       "3       10182018\n",
       "4       10182018\n",
       "          ...   \n",
       "3018    10072018\n",
       "3019    10132018\n",
       "3020    10122018\n",
       "3021    10102018\n",
       "3022    10122018\n",
       "Name: Date, Length: 3023, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = df[\"Date\"].squeeze()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>06</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  day month  year\n",
       "0  10    14  2018\n",
       "1  10    06  2018\n",
       "2  10    10  2018\n",
       "3  10    18  2018\n",
       "4  10    18  2018"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = df[\"Date\"].squeeze()\n",
    "#a= s.apply(lambda x: 'date'= x,'day':x[:\n",
    "day = s.apply(lambda x: str(x)[0:2])\n",
    "month = s.apply(lambda x:str(x)[2:4])\n",
    "year = s.apply(lambda x:str(x)[4:])\n",
    "df['day']= day\n",
    "df['month'] =month\n",
    "df['year'] = year\n",
    "\n",
    "df[['day','month','year']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "You can create new features based on the features you have. These might be more useful for your (future) machine learning model than the ones that are already present in the dataset.\n",
    "In this squirrel dataset, most of the fields encode the action taken by the squirrel when being approached by the human.\n",
    "We will combine them into a single feature `Reaction` with values `'yes'` and `'no'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Hectare</th>\n",
       "      <th>Shift</th>\n",
       "      <th>Date</th>\n",
       "      <th>Hectare Squirrel Number</th>\n",
       "      <th>Age</th>\n",
       "      <th>Primary Fur Color</th>\n",
       "      <th>Highlight Fur Color</th>\n",
       "      <th>Combination of Primary and Highlight Color</th>\n",
       "      <th>...</th>\n",
       "      <th>Tail flags</th>\n",
       "      <th>Tail twitches</th>\n",
       "      <th>Approaches</th>\n",
       "      <th>Indifferent</th>\n",
       "      <th>Runs from</th>\n",
       "      <th>Other Interactions</th>\n",
       "      <th>Lat/Long</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-73.956134</td>\n",
       "      <td>40.794082</td>\n",
       "      <td>37F</td>\n",
       "      <td>PM</td>\n",
       "      <td>10142018</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POINT (-73.9561344937861 40.7940823884086)</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-73.957044</td>\n",
       "      <td>40.794851</td>\n",
       "      <td>37E</td>\n",
       "      <td>PM</td>\n",
       "      <td>10062018</td>\n",
       "      <td>3</td>\n",
       "      <td>Adult</td>\n",
       "      <td>Gray</td>\n",
       "      <td>Cinnamon</td>\n",
       "      <td>Gray+Cinnamon</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>me</td>\n",
       "      <td>POINT (-73.9570437717691 40.794850940803904)</td>\n",
       "      <td>10</td>\n",
       "      <td>06</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-73.976831</td>\n",
       "      <td>40.766718</td>\n",
       "      <td>02E</td>\n",
       "      <td>AM</td>\n",
       "      <td>10102018</td>\n",
       "      <td>3</td>\n",
       "      <td>Adult</td>\n",
       "      <td>Cinnamon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cinnamon+</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POINT (-73.9768311751004 40.76671780725581)</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.975725</td>\n",
       "      <td>40.769703</td>\n",
       "      <td>05D</td>\n",
       "      <td>PM</td>\n",
       "      <td>10182018</td>\n",
       "      <td>5</td>\n",
       "      <td>Juvenile</td>\n",
       "      <td>Gray</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gray+</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POINT (-73.9757249834141 40.7697032606755)</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.959313</td>\n",
       "      <td>40.797533</td>\n",
       "      <td>39B</td>\n",
       "      <td>AM</td>\n",
       "      <td>10182018</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POINT (-73.9593126695714 40.797533370163)</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X          Y Hectare Shift      Date  Hectare Squirrel Number  \\\n",
       "0 -73.956134  40.794082     37F    PM  10142018                        3   \n",
       "1 -73.957044  40.794851     37E    PM  10062018                        3   \n",
       "2 -73.976831  40.766718     02E    AM  10102018                        3   \n",
       "3 -73.975725  40.769703     05D    PM  10182018                        5   \n",
       "4 -73.959313  40.797533     39B    AM  10182018                        1   \n",
       "\n",
       "        Age Primary Fur Color Highlight Fur Color  \\\n",
       "0       NaN               NaN                 NaN   \n",
       "1     Adult              Gray            Cinnamon   \n",
       "2     Adult          Cinnamon                 NaN   \n",
       "3  Juvenile              Gray                 NaN   \n",
       "4       NaN               NaN                 NaN   \n",
       "\n",
       "  Combination of Primary and Highlight Color  ... Tail flags Tail twitches  \\\n",
       "0                                          +  ...      False         False   \n",
       "1                              Gray+Cinnamon  ...      False         False   \n",
       "2                                  Cinnamon+  ...      False         False   \n",
       "3                                      Gray+  ...      False         False   \n",
       "4                                          +  ...      False         False   \n",
       "\n",
       "  Approaches Indifferent  Runs from  Other Interactions  \\\n",
       "0      False       False      False                 NaN   \n",
       "1      False       False       True                  me   \n",
       "2      False        True      False                 NaN   \n",
       "3      False       False       True                 NaN   \n",
       "4      False       False      False                 NaN   \n",
       "\n",
       "                                       Lat/Long  day  month  year  \n",
       "0    POINT (-73.9561344937861 40.7940823884086)   10     14  2018  \n",
       "1  POINT (-73.9570437717691 40.794850940803904)   10     06  2018  \n",
       "2   POINT (-73.9768311751004 40.76671780725581)   10     10  2018  \n",
       "3    POINT (-73.9757249834141 40.7697032606755)   10     18  2018  \n",
       "4     POINT (-73.9593126695714 40.797533370163)   10     18  2018  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        no\n",
       "1       yes\n",
       "2        no\n",
       "3       yes\n",
       "4       yes\n",
       "       ... \n",
       "3018    yes\n",
       "3019     no\n",
       "3020     no\n",
       "3021     no\n",
       "3022    yes\n",
       "Name: Reaction, Length: 3023, dtype: object"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reaction_columns = ['Kuks', 'Quaas', 'Moans', 'Tail flags',\n",
    "                   'Tail twitches', 'Approaches', 'Runs from',\n",
    "                   'Other Interactions']\n",
    "\n",
    "df['Reaction'] = df[reaction_columns].any(axis=1)\n",
    "df['Reaction'] = df['Reaction'].apply(lambda x : \"yes\" if x else \"no\")\n",
    "df['Reaction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A important step for a data processing pipeline is making the data understandable for machine learning algorithms. Most of them do not understand strings, like `yes` and `no` in our newly created column.\n",
    "We need to transform them to a binary format so that the machine learning model can take advantage of that feature.We are going to **One Hot Encode** our feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reaction_no</th>\n",
       "      <th>Reaction_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3018</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3019</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3020</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3021</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3023 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Reaction_no  Reaction_yes\n",
       "0               1             0\n",
       "1               0             1\n",
       "2               1             0\n",
       "3               0             1\n",
       "4               0             1\n",
       "...           ...           ...\n",
       "3018            0             1\n",
       "3019            1             0\n",
       "3020            1             0\n",
       "3021            1             0\n",
       "3022            0             1\n",
       "\n",
       "[3023 rows x 2 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(df.Reaction, prefix='Reaction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we have a redundancy here, as we could just transform `'yes'` to `1` and `'no'` to `0` in our `Reaction` column. This can be done by setting the argument `drop_first` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3018</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3019</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3020</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3021</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3023 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Reaction\n",
       "0            0\n",
       "1            1\n",
       "2            0\n",
       "3            1\n",
       "4            1\n",
       "...        ...\n",
       "3018         1\n",
       "3019         0\n",
       "3020         0\n",
       "3021         0\n",
       "3022         1\n",
       "\n",
       "[3023 rows x 1 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.get_dummies(df.Reaction, prefix='Reaction', drop_first=\"True\")\n",
    "df.rename(columns={\"Reaction_yes\" : \"Reaction\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Similar things can be done after converting the data frame to an array using the `scikit-learn` library with `LabelBinarizer` or `OneHotEncoder`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature normalization or standardization\n",
    "Although they are sometimes used interchangeably, normalization and standardization are two different ways to bring a column of values to a common scale.\n",
    "In this section, we're going to use the word normalization to refer to this concept.\n",
    "\n",
    "Why do we normalize data ?\n",
    "\n",
    "*For example, assume your input dataset contains one column with values ranging from 0 to 1, and another column with values ranging from 10,000 to 100,000. The great difference in the scale of the numbers could cause problems when you attempt to combine the values as features during modeling.* [[Source]](https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/normalize-data)\n",
    "\n",
    "Some algorithms require that data be normalized before training a model. Other algorithms perform their own data scaling or normalization.\n",
    "\n",
    "Given a column of values *x*, if we choose to scale them, there are a few options:\n",
    "\n",
    "- Normalization, also called *min-max scaling*, rescales every value to a range between [0, 1]. The maximum and the minimum are computed for each column separately.\n",
    "\n",
    "  $$ z = \\frac{x - min(x)}{max(x) - min(x)} $$\n",
    "\n",
    "- Standardization, also called z-score normalization, rescales the value around a 0 mean and a standard deviation of 1. It essentially transforms all values of *x* to a *z-score*. Mean and standard deviation are computed for each column separately.\n",
    "\n",
    "$$ z = \\frac{x - mean(x)}{std(x)} $$\n",
    "\n",
    "\n",
    "### Be careful!\n",
    "\n",
    "When you want to normalize your dataset, you have to do so **AFTER** splitting your data into different train-test splits. Indeed, normalizing your data before would use some information from your testing set in the training set, thus biasing the model.\n",
    "Indeed, in a real world scenario, you would not have access to the testing set, as this would be the data that you are meant to predict.\n",
    "\n",
    "The procedure is the following:\n",
    "1. Split your data into train and test\n",
    "2. For every variable $x$ of your **training set**, compute $max(x_{train})$ and $min(x_{train})$ , or $mean(x_{train})$ and $std(x_{train})$ depending if you do min-max scaling or z-score-normalization.\n",
    "3. Normalize your training set and your testing set using these values (here I'm only showing the testing set).\n",
    "$$ z_{test} = \\frac{x_{test} - min(x_{train})}{max(x_{train}) - min(x_{train})} $$\n",
    "\n",
    "\n",
    "$$ z_{test} = \\frac{x_{test} - mean(x_{train})}{std(x_{train})} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "dataset = datasets.load_breast_cancer()\n",
    "X, y = dataset.data, dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n",
      "<class 'numpy.ndarray'>\n",
      "[[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset))\n",
    "print(type(X))\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
      "        1.189e-01],\n",
      "       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
      "        8.902e-02],\n",
      "       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
      "        8.758e-02],\n",
      "       ...,\n",
      "       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
      "        7.820e-02],\n",
      "       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
      "        1.240e-01],\n",
      "       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
      "        7.039e-02]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
      "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
      "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
      "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
      "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
      "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
      "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]), 'frame': None, 'target_names': array(['malignant', 'benign'], dtype='<U9'), 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry\\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        worst/largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n        10 is Radius SE, field 20 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. topic:: References\\n\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.', 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
      "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
      "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
      "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
      "       'smoothness error', 'compactness error', 'concavity error',\n",
      "       'concave points error', 'symmetry error',\n",
      "       'fractal dimension error', 'worst radius', 'worst texture',\n",
      "       'worst perimeter', 'worst area', 'worst smoothness',\n",
      "       'worst compactness', 'worst concavity', 'worst concave points',\n",
      "       'worst symmetry', 'worst fractal dimension'], dtype='<U23'), 'filename': 'breast_cancer.csv', 'data_module': 'sklearn.datasets.data'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30)\n"
     ]
    }
   ],
   "source": [
    "# Splitting data into train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=41, test_size=0.2)\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.283e+01 1.573e+01 8.289e+01 ... 9.783e-02 3.006e-01 7.802e-02]\n",
      " [1.426e+01 1.817e+01 9.122e+01 ... 7.530e-02 2.636e-01 7.676e-02]\n",
      " [1.365e+01 1.316e+01 8.788e+01 ... 8.056e-02 2.380e-01 8.718e-02]\n",
      " ...\n",
      " [1.375e+01 2.377e+01 8.854e+01 ... 6.106e-02 2.663e-01 6.321e-02]\n",
      " [2.016e+01 1.966e+01 1.311e+02 ... 1.425e-01 3.055e-01 5.933e-02]\n",
      " [1.145e+01 2.097e+01 7.381e+01 ... 6.127e-02 2.762e-01 8.851e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train[0].sum()/len(X_train[0])\n",
    "len(X_train[0])\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll want you to use numpy's mean and standard deviation functions to standardize each feature of the training and testing set. These are `np.mean()` and `np.std()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 'mean radius' has mean -0.00 and standard deviation 1.00\n",
      "Feature 'mean texture' has mean 0.00 and standard deviation 1.00\n",
      "Feature 'mean perimeter' has mean -0.00 and standard deviation 1.00\n",
      "Feature 'mean area' has mean -0.00 and standard deviation 1.00\n",
      "Feature 'mean smoothness' has mean -0.00 and standard deviation 1.00\n",
      "Feature 'mean compactness' has mean 0.00 and standard deviation 1.00\n",
      "Feature 'mean concavity' has mean -0.00 and standard deviation 1.00\n",
      "Feature 'mean concave points' has mean -0.00 and standard deviation 1.00\n",
      "Feature 'mean symmetry' has mean 0.00 and standard deviation 1.00\n",
      "Feature 'mean fractal dimension' has mean 0.00 and standard deviation 1.00\n",
      "Feature 'radius error' has mean -0.00 and standard deviation 1.00\n",
      "Feature 'texture error' has mean -0.00 and standard deviation 1.00\n",
      "Feature 'perimeter error' has mean -0.00 and standard deviation 1.00\n",
      "Feature 'area error' has mean 0.00 and standard deviation 1.00\n",
      "Feature 'smoothness error' has mean -0.00 and standard deviation 1.00\n",
      "Feature 'compactness error' has mean -0.00 and standard deviation 1.00\n",
      "Feature 'concavity error' has mean -0.00 and standard deviation 1.00\n",
      "Feature 'concave points error' has mean -0.00 and standard deviation 1.00\n",
      "Feature 'symmetry error' has mean 0.00 and standard deviation 1.00\n",
      "Feature 'fractal dimension error' has mean 0.00 and standard deviation 1.00\n",
      "Feature 'worst radius' has mean 0.00 and standard deviation 1.00\n",
      "Feature 'worst texture' has mean -0.00 and standard deviation 1.00\n",
      "Feature 'worst perimeter' has mean -0.00 and standard deviation 1.00\n",
      "Feature 'worst area' has mean -0.00 and standard deviation 1.00\n",
      "Feature 'worst smoothness' has mean 0.00 and standard deviation 1.00\n",
      "Feature 'worst compactness' has mean -0.00 and standard deviation 1.00\n",
      "Feature 'worst concavity' has mean 0.00 and standard deviation 1.00\n",
      "Feature 'worst concave points' has mean -0.00 and standard deviation 1.00\n",
      "Feature 'worst symmetry' has mean -0.00 and standard deviation 1.00\n",
      "Feature 'worst fractal dimension' has mean 0.00 and standard deviation 1.00\n",
      "[[-0.43790257 -0.21350264 -0.50001024 ... -1.62194722 -0.65818251\n",
      "  -1.11659911]\n",
      " [ 0.19791867 -1.06494654  0.13682124 ...  0.15746166  0.34259051\n",
      "  -0.13890569]\n",
      " [ 0.16291934 -0.94862907  0.17492951 ...  0.38079026  0.41218069\n",
      "   0.02033139]\n",
      " ...\n",
      " [-0.16665773 -0.09951152 -0.14983761 ...  0.46645055 -0.39142016\n",
      "  -0.41784897]\n",
      " [-0.88122747 -0.51127537 -0.86330905 ... -0.30082087 -0.20253254\n",
      "   1.80590246]\n",
      " [-0.06165972 -0.72297317 -0.13205375 ... -0.90625553 -0.89512049\n",
      "  -0.73687991]]\n"
     ]
    }
   ],
   "source": [
    "# Standardizing each feature using the train mean and standard deviation\n",
    "\n",
    "for idx, name in enumerate(dataset.feature_names):\n",
    "    # Get mean and standard deviation from training set (per feature)\n",
    "    \n",
    "     mean=np.mean(X_train[:,idx])\n",
    "     stdev = np.std(X_train[:,idx])\n",
    "     \n",
    "\n",
    "     print(f\"Feature '{name}' has mean {mean:.2f} and standard deviation {stdev:.2f}\")\n",
    "    # Standardize training and testing set using the mean and standard deviation from the training set\n",
    "     z = (X_train[:,idx] -mean)/stdev\n",
    "     z_test =(X_test[:,idx]-mean)/stdev\n",
    "    \n",
    "     X_train[:,idx] = z\n",
    "     X_test[:,idx]= z_test\n",
    "#print(X_train)\n",
    "print(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the previous cell twice (without running the others cells again), you'll see that the second time, the mean and standard deviation for each feature will be 0 and 1 respectively, which is exactly what we want when we standardize (z-score normalization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "\n",
    "Sometimes, when you have multiple classes and the number of samples of each class are not equally distributed, i.e. there is an imbalance in the number of samples of each class, you can resort to resampling.\n",
    "Resampling is using more (or less) of a given class to get a balanced dataset.\n",
    "\n",
    "**BE CAREFUL**, resample **AFTER** splitting your data set into two parts. You do not want to accidentally have a copy of a testing sample in the training set.\n",
    "Moreover, **do not resample the testing set**. This would give a false sense of the performance of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "dataset = datasets.load_breast_cancer()\n",
    "X, y = dataset.data, dataset.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of class 0 samples :  (212, 30)\n",
      "Shape of class 1 samples :  (357, 30)\n"
     ]
    }
   ],
   "source": [
    "# We separate the samples of the different classes\n",
    "class_one_idx = np.argwhere(y==1)\n",
    "class_zero_idx = np.argwhere(y==0)\n",
    "\n",
    "class_one_x = np.squeeze(X[class_one_idx])\n",
    "class_zero_x = np.squeeze(X[class_zero_idx])\n",
    "\n",
    "print(\"Shape of class 0 samples : \", class_zero_x.shape)\n",
    "print(\"Shape of class 1 samples : \", class_one_x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that we have 212 samples of class 0 and 357 samples of class 1.\n",
    "We can either upsample, i.e. take more samples of, the minority class (here class 0) or we can downsample, i.e. take fewer samples of, the majority class (here class 1).\n",
    "To do this, we first have to separate the samples of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape of class 0 samples:  (357, 30)\n",
      "New shape of class 1 samples:  (212, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Upsample minority class\n",
    "class_zero_upsampled = resample(class_zero_x, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=357,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "print(\"New shape of class 0 samples: \",class_zero_upsampled.shape)\n",
    "\n",
    "# Downsample majority class\n",
    "class_one_downsampled = resample(class_one_x, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=212,    # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "print(\"New shape of class 1 samples: \",class_one_downsampled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having either upsampled our minority class, or downsampled our majority class, we can combine the upsampled with the majority class or the downsampled with the minority class to have a balanced data set.\n",
    "\n",
    "Which one you use depends on what you want to do, and which one does best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Upsampled) Balanced data set shape :  (714, 30)\n",
      "(Downsampled) Balanced data set shape :  (424, 30)\n"
     ]
    }
   ],
   "source": [
    "X_balanced = np.concatenate((class_zero_upsampled, class_one_x), axis=0)\n",
    "print(\"(Upsampled) Balanced data set shape : \", X_balanced.shape)\n",
    "\n",
    "X_balanced = np.concatenate((class_one_downsampled, class_zero_x), axis=0)\n",
    "print(\"(Downsampled) Balanced data set shape : \", X_balanced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading material and additional ressources\n",
    "\n",
    "[[1] Feature Engineering - Elite Data Science](https://elitedatascience.com/feature-engineering)  \n",
    "[[2] Feature Engineering - Towards Data Science](https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114)  \n",
    "[[3] Feature Engineering Tutorial - Kaggle](https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114)  \n",
    "[[4] LabelBinarizer - scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)  \n",
    "[[5] OneHotEncoder - scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)  \n",
    "[[6] Zscore - Simply Psychology](https://www.simplypsychology.org/z-score.html)  \n",
    "[[7] Normalize data - Microsoft Azure](https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/normalize-data)  \n",
    "[[8] How to handle imbalanced classes - Elite Data Science ](https://elitedatascience.com/imbalanced-classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('real_estate_prediction')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9228d34919f745069d3ef84d5d121bbcb666b039c72f040aacee9f3bccaf6b85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
